{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbbb110-ef8e-43b9-9afd-25db7c4db47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de3842-1545-4c0c-813f-b29542d0d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pde.models.conv_lstm import ConvLSTM_Model\n",
    "from pde.train_utils.dataloader_openstl import load_data\n",
    "from pde.train_utils.losses import calculate_pde_and_continuity_loss, weighted_mae_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52771ce-667b-48f7-829e-8b9f9e52f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/fa.buzaev/data_5/\"\n",
    "last_hour_in = 13\n",
    "last_hour_out = 25\n",
    "exp_root = 'exp_convlstm'\n",
    "model_root = f\"/home/asutkin/kursach/{exp_root}/best_models/model_mistake.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c61272-6158-4caf-875a-8b33c3b51c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asutkin/.conda/envs/vrwkv/lib/python3.11/site-packages/xarray/core/accessor_dt.py:441: FutureWarning: dt.weekofyear and dt.week have been deprecated. Please use dt.isocalendar().week instead.\n",
      "  warnings.warn(\n",
      "/home/asutkin/.conda/envs/vrwkv/lib/python3.11/site-packages/xarray/core/accessor_dt.py:441: FutureWarning: dt.weekofyear and dt.week have been deprecated. Please use dt.isocalendar().week instead.\n",
      "  warnings.warn(\n",
      "/home/asutkin/.conda/envs/vrwkv/lib/python3.11/site-packages/xarray/core/accessor_dt.py:441: FutureWarning: dt.weekofyear and dt.week have been deprecated. Please use dt.isocalendar().week instead.\n",
      "  warnings.warn(\n",
      "/home/asutkin/.conda/envs/vrwkv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "dataloader_train, dataloader_vali, dataloader_test, mean, std = load_data(batch_size=12,\n",
    "                                                                          val_batch_size=12,\n",
    "                                                                          data_root=data_root,\n",
    "                                                                          num_workers=6,\n",
    "                                                                          # data_split='2_8125',\n",
    "                                                                          # data_name='mv3',\n",
    "                                                                          data_split='5_625',\n",
    "                                                                          data_name='uv10',\n",
    "                                                                          # data_name='mv6',\n",
    "                                                                          # train_time=['1979', '2015'],\n",
    "                                                                          train_time=['2010', '2011'],\n",
    "                                                                          val_time=['2016', '2016'],\n",
    "                                                                          test_time=['2017', '2018'],\n",
    "                                                                          idx_in=[*range(1, last_hour_in)],\n",
    "                                                                          idx_out=[*range(last_hour_in + 12, last_hour_out + 12)],\n",
    "                                                                          step=1,\n",
    "                                                                          level=1,\n",
    "                                                                          distributed=False, use_augment=False, \n",
    "                                                                          use_prefetcher=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8304b0-4b7d-46ba-a2d6-f8ed6c054f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import sys, os\n",
    "\n",
    "criterion = torch.nn.L1Loss()\n",
    "configs = {\n",
    "    'in_shape': [12, 2, 32, 64],\n",
    "    'filter_size': 5,\n",
    "    'patch_size': 1,\n",
    "    'stride': 1,\n",
    "    'pre_seq_length': 12,\n",
    "    'aft_seq_length': 1,\n",
    "    'layer_norm': 0,\n",
    "    \n",
    "}\n",
    "model = ConvLSTM_Model(1, [128, 128, 128, 128], configs).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "checkpoint = torch.load(model_root)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "best_loss = checkpoint['loss']\n",
    "# best_loss=float('inf')\n",
    "with open(f\"/home/asutkin/kursach/{exp_root}/logs_mistake.txt\", 'a') as file:\n",
    "    for i in range(4, 5):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for x_train, y_train in tqdm(dataloader_train, desc='Training'):\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            big_loss = 0\n",
    "            for j in range(1, last_hour_out - last_hour_in):\n",
    "                time_tensor = j * torch.ones(x_train.shape[0], device=x_train.device).unsqueeze(-1)\n",
    "                prediction = model(x_train, time_tensor)\n",
    "                loss = criterion(prediction[:,0,:,:,:], y_train[:,j,:,:,:])\n",
    "                x_train = torch.cat((x_train[:,1:,:,:,:], prediction[:,0,:,:,:].unsqueeze(1)), dim=1)\n",
    "                big_loss += loss\n",
    "            big_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += big_loss.item() / (last_hour_out - last_hour_in)\n",
    "            # torch.cuda.empty_cache()\n",
    "        train_loss /= len(dataloader_train)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        for x_val, y_val in tqdm(dataloader_vali, desc='Validating'):\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            big_loss_val = 0\n",
    "            for j in range(1, last_hour_out - last_hour_in):\n",
    "                time_tensor = j * torch.ones(x_val.shape, device=x_val.device).unsqueeze(-1)\n",
    "                prediction = model(x_val, time_tensor)\n",
    "                loss = criterion(prediction[:,0,:,:,:], y_val[:,j,:,:,:])\n",
    "                x_val = torch.cat((x_val[:,1:,:,:,:], prediction[:,0,:,:,:].unsqueeze(1)), dim=1)\n",
    "                big_loss_val += loss\n",
    "            val_loss += big_loss_val.item() / (last_hour_out - last_hour_in)\n",
    "        val_loss /= len(dataloader_vali)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, model_root)\n",
    "\n",
    "        print(f\"Epoch: {i + 1}; Train_loss: {train_loss}; Vall_loss: {val_loss}\")\n",
    "        log = json.dumps({'Epoch': i+1, 'Train_loss': train_loss, 'Val_loss': val_loss})\n",
    "        file.write(f\"{log}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0e4f5-70d4-460a-8e59-61ac25a2b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "def denorm(item, std, mean, idx=0):\n",
    "    mean = mean.squeeze()[idx]\n",
    "    std = std.squeeze()[idx]\n",
    "    item_denorm = item * std + mean\n",
    "    return item_denorm\n",
    "\n",
    "for x_test, y_test in dataloader_test:\n",
    "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "    break\n",
    "    \n",
    "x_test_ = torch.empty(x_test.shape, device=device)\n",
    "\n",
    "configs = {\n",
    "    'in_shape': [12, 2, 32, 64],\n",
    "    'filter_size': 5,\n",
    "    'patch_size': 1,\n",
    "    'stride': 1,\n",
    "    'pre_seq_length': 12,\n",
    "    'aft_seq_length': 1,\n",
    "    'layer_norm': 0,\n",
    "    \n",
    "}\n",
    "model = ConvLSTM_Model(1, [128, 128, 128, 128], configs)\n",
    "criterion = torch.nn.MSELoss()\n",
    "checkpoint = torch.load(model_root)\n",
    "model.load_state_dict(checkpoint['model']).to(device)\n",
    "model.eval()\n",
    "t = 0\n",
    "x_data = x_test\n",
    "\n",
    "for i in tqdm(range(1, 73)):\n",
    "    time_tensor = i * torch.ones(x_test.shape[0], device=x_test.device).unsqueeze(-1)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(x_data, time_tensor)\n",
    "    plt.figure(constrained_layout=True, figsize=(32, 6))\n",
    "    x_data = torch.cat((x_data[:,1:,:,:,:], prediction.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(denorm(prediction[7, 0], std, mean).squeeze().detach().cpu().numpy())\n",
    "    plt.title(f\"Prediction U wind by Andrew, step={t}\")\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(denorm(y_test[7, t, 0], std, mean).squeeze().detach().cpu().numpy())\n",
    "    plt.title(f\"True answer, step={t}\")\n",
    "    plt.colorbar(boundaries=np.linspace(-20, 20, 20)) \n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(np.abs(denorm(prediction[7, 0], std, mean).squeeze().detach().cpu().numpy() - denorm(y_test[7, t, 0], std, mean).squeeze().detach().cpu().numpy()))\n",
    "    plt.title(f\"Absolute difference, step={t}\")\n",
    "    plt.colorbar(boundaries=np.linspace(0, 20, 20))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    plt.savefig(f\"/home/asutkin/kursach/{exp_root}/predictions/imvp_{t}.png\")\n",
    "\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1f995-286d-4a02-ab79-e2c81fb52b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "images = []\n",
    "for i in range(0,72):\n",
    "    images.append(imageio.imread(f\"/home/asutkin/kursach/{exp_root}/predictions/imvp_{i}.png\"))\n",
    "\n",
    "\n",
    "imageio.mimsave(f\"/home/asutkin/kursach/{exp_root}/predictions/wind_norm.gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f81fc-9e7b-4956-a3fb-de2312dc9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(constrained_layout=True, figsize=(32, 6))\n",
    "\n",
    "plt.imshow(prediction[65, 0].squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b87fb-f6bb-430a-9812-ed4715b1439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(constrained_layout=True, figsize=(32, 6))\n",
    "plt.imshow(y_test[65, 1, 0].squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd10f80-c27c-4abb-9bc6-55c324940250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.figure(constrained_layout=True, figsize=(32, 6))\n",
    "plt.imshow(np.abs(y_test[65, 0, 0].squeeze().detach().cpu().numpy() - prediction[65, 0].squeeze().detach().cpu().numpy()))\n",
    "plt.colorbar(boundaries=np.linspace(-20, 20, 20)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63189d2-cd25-434d-80fe-f9c44c0c8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}, '/home/asutkin/kursach/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f78340-b257-4d33-b717-759a8a77fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1457/1457 [09:32<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from pde.train_utils.losses import calculate_pde_and_continuity_loss, weighted_rmse_torch\n",
    "\n",
    "def denorm(item, std, mean, idx=0):\n",
    "    mean = mean.squeeze()[idx]\n",
    "    std = std.squeeze()[idx]\n",
    "    item_denorm = item * std + mean\n",
    "    return item_denorm\n",
    "\n",
    "configs = {\n",
    "    'in_shape': [12, 2, 32, 64],\n",
    "    'filter_size': 5,\n",
    "    'patch_size': 1,\n",
    "    'stride': 1,\n",
    "    'pre_seq_length': 12,\n",
    "    'aft_seq_length': 1,\n",
    "    'layer_norm': 0,\n",
    "    \n",
    "}\n",
    "model = ConvLSTM_Model(1, [128, 128, 128, 128], configs).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "checkpoint = torch.load(model_root)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "for x_test, y_test in tqdm(dataloader_test):\n",
    "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "    t = 0\n",
    "    x_data = x_test\n",
    "    array_to_plot = np.empty((last_hour_out - last_hour_in))\n",
    "    for j in range(1, last_hour_out - last_hour_in):\n",
    "        time_tensor = j * torch.ones(x_test.shape[0], device=x_test.device).unsqueeze(-1)\n",
    "        prediction = model(x_test, time_tensor)\n",
    "        x_test = torch.cat((x_test[:,1:,:,:,:], prediction[:,0,:,:,:].unsqueeze(1)), dim=1)\n",
    "\n",
    "        u_wind_prediction = denorm(prediction[:,0,0,:,:], std, mean)\n",
    "        metric_u_wind = weighted_rmse_torch(u_wind_prediction.unsqueeze(1), denorm(y_test[:, t, 0], std, mean).unsqueeze(1))\n",
    "\n",
    "        v_wind_prediction = denorm(prediction[:,0,1,:,:], std, mean, idx=1)\n",
    "        metric_v_wind = weighted_rmse_torch(v_wind_prediction.unsqueeze(1), denorm(y_test[:, t, 1], std, mean, idx=1).unsqueeze(1))\n",
    "\n",
    "        metric = (metric_u_wind + metric_v_wind) / 2\n",
    "\n",
    "        array_to_plot[t] = metric.detach().cpu().item()\n",
    "        t += 1\n",
    "    \n",
    "    all_batches.append(array_to_plot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-vrwkv]",
   "language": "python",
   "name": "conda-env-.conda-vrwkv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
